{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-0fo1tch5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-0fo1tch5\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 9e8c38aa5b92bbf0e20f65fc611fd43b43196859\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alpaca-py<0.38,>=0.37 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (0.37.0)\n",
      "Requirement already satisfied: alpaca-trade-api<4,>=3 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (3.2.0)\n",
      "Requirement already satisfied: ccxt<4,>=3 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (3.1.60)\n",
      "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-hd917lte/elegantrl_bebcbdd6e3c843a4a6a418dcca6adfba\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-hd917lte/elegantrl_bebcbdd6e3c843a4a6a418dcca6adfba\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 37aac1f592e1add9f9fd37ae8db1094656009b76\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jqdatasdk<2,>=1 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (1.9.7)\n",
      "Requirement already satisfied: pandas-market-calendars<6,>=5 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (5.1.1)\n",
      "Requirement already satisfied: pyfolio-reloaded<0.10,>=0.9 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (0.9.9)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (1.5.6)\n",
      "Requirement already satisfied: ray<3,>=2 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.47.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (1.7.0)\n",
      "Requirement already satisfied: selenium<5,>=4 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (4.32.0)\n",
      "Requirement already satisfied: stable-baselines3>=2.0.0a5 in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.7.0a0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (0.5.4)\n",
      "Requirement already satisfied: webdriver-manager<5,>=4 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (4.0.2)\n",
      "Requirement already satisfied: wrds<4,>=3 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (3.3.0)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from finrl==0.3.8) (0.2.58)\n",
      "Requirement already satisfied: th in /home/feng/CS/venv/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: numpy in /home/feng/CS/venv/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (2.2.6)\n",
      "Requirement already satisfied: gymnasium in /home/feng/CS/venv/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: matplotlib in /home/feng/CS/venv/lib/python3.12/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (3.10.3)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.0.3)\n",
      "Requirement already satisfied: pandas>=1.5.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.4)\n",
      "Requirement already satisfied: sseclient-py<2.0.0,>=1.7.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (10.4)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.26.20)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.12.13)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (6.0.1)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.1.0)\n",
      "Requirement already satisfied: packaging in /home/feng/CS/venv/lib/python3.12/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (1.20.1)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (80.9.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /home/feng/CS/venv/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.6.15)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (45.0.4)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from ccxt<4,>=3->finrl==0.3.8) (3.5.0)\n",
      "Requirement already satisfied: six in /home/feng/CS/venv/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.17.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in /home/feng/CS/venv/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.41)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /home/feng/CS/venv/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: thriftpy2!=0.5.1,>=0.3.9 in /home/feng/CS/venv/lib/python3.12/site-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (0.5.2)\n",
      "Requirement already satisfied: tzdata in /home/feng/CS/venv/lib/python3.12/site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: python-dateutil in /home/feng/CS/venv/lib/python3.12/site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (2.9.0.post0)\n",
      "Requirement already satisfied: exchange-calendars>=3.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from pandas-market-calendars<6,>=5->finrl==0.3.8) (4.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.1)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (9.4.0)\n",
      "Requirement already satisfied: pytz>=2014.10 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2025.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.16.0)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
      "Requirement already satisfied: empyrical-reloaded>=0.5.9 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.5.12)\n",
      "Requirement already satisfied: cvxpy>=1.1.19 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.6)\n",
      "Requirement already satisfied: ecos<3.0.0,>=2.0.14 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (2.0.14)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.2.1)\n",
      "Requirement already satisfied: filelock in /home/feng/CS/venv/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.18.0)\n",
      "Requirement already satisfied: jsonschema in /home/feng/CS/venv/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.24.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (5.29.5)\n",
      "Requirement already satisfied: aiohttp_cors in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.8.1)\n",
      "Requirement already satisfied: colorful in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.5.6)\n",
      "Requirement already satisfied: py-spy>=0.4.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.73.1)\n",
      "Requirement already satisfied: opencensus in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.11.4)\n",
      "Requirement already satisfied: opentelemetry-sdk in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-prometheus in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-proto in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.34.1)\n",
      "Requirement already satisfied: prometheus_client>=0.7.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.22.1)\n",
      "Requirement already satisfied: smart_open in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (7.1.0)\n",
      "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.31.2)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2.6.4)\n",
      "Requirement already satisfied: pyarrow>=9.0.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (20.0.0)\n",
      "Requirement already satisfied: fsspec in /home/feng/CS/venv/lib/python3.12/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2025.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/feng/CS/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/feng/CS/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.10)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
      "Requirement already satisfied: trio~=0.17 in /home/feng/CS/venv/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/feng/CS/venv/lib/python3.12/site-packages (from selenium<5,>=4->finrl==0.3.8) (0.12.2)\n",
      "Requirement already satisfied: sortedcontainers in /home/feng/CS/venv/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
      "Requirement already satisfied: outcome in /home/feng/CS/venv/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/feng/CS/venv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/feng/CS/venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in /home/feng/CS/venv/lib/python3.12/site-packages (from webdriver-manager<5,>=4->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/feng/CS/venv/lib/python3.12/site-packages (from wrds<4,>=3->finrl==0.3.8) (2.9.10)\n",
      "Requirement already satisfied: greenlet>=1 in /home/feng/CS/venv/lib/python3.12/site-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.2.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /home/feng/CS/venv/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.3.8)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in /home/feng/CS/venv/lib/python3.12/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.11.4)\n",
      "Requirement already satisfied: pycares>=4.9.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8) (4.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.7)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/feng/CS/venv/lib/python3.12/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/feng/CS/venv/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.22)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.4)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.11.1)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /home/feng/CS/venv/lib/python3.12/site-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
      "Requirement already satisfied: bottleneck>=1.3.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.5.0)\n",
      "Requirement already satisfied: pyluach in /home/feng/CS/venv/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (2.2.0)\n",
      "Requirement already satisfied: toolz in /home/feng/CS/venv/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (1.0.0)\n",
      "Requirement already satisfied: korean_lunar_calendar in /home/feng/CS/venv/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas-market-calendars<6,>=5->finrl==0.3.8) (0.3.1)\n",
      "Requirement already satisfied: decorator in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/feng/CS/venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/feng/CS/venv/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /home/feng/CS/venv/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from matplotlib->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (3.2.3)\n",
      "Requirement already satisfied: jinja2 in /home/feng/CS/venv/lib/python3.12/site-packages (from osqp>=0.6.2->cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.1.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/feng/CS/venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.7.1)\n",
      "Requirement already satisfied: cloudpickle in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from gymnasium->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (0.0.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.3.1)\n",
      "Requirement already satisfied: opencv-python in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.19.0)\n",
      "Requirement already satisfied: psutil in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.67.1)\n",
      "Requirement already satisfied: rich in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (14.0.0)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.11.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/feng/CS/venv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
      "Requirement already satisfied: Cython>=3.0.10 in /home/feng/CS/venv/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.1.2)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/feng/CS/venv/lib/python3.12/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8) (0.3.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/feng/CS/venv/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.25.1)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /home/feng/CS/venv/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.70.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.40.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/feng/CS/venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n",
      "Requirement already satisfied: opentelemetry-api~=1.12 in /home/feng/CS/venv/lib/python3.12/site-packages (from opentelemetry-exporter-prometheus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.34.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from opentelemetry-api~=1.12->opentelemetry-exporter-prometheus->ray[default,tune]<3,>=2->finrl==0.3.8) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/feng/CS/venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api~=1.12->opentelemetry-exporter-prometheus->ray[default,tune]<3,>=2->finrl==0.3.8) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /home/feng/CS/venv/lib/python3.12/site-packages (from opentelemetry-sdk->ray[default,tune]<3,>=2->finrl==0.3.8) (0.55b1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/feng/CS/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.2)\n",
      "Requirement already satisfied: wrapt in /home/feng/CS/venv/lib/python3.12/site-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.8) (1.17.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/feng/CS/venv/lib/python3.12/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/feng/CS/venv/lib/python3.12/site-packages (from stack_data->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.3)\n",
      "Requirement already satisfied: niltype<2.0,>=0.3 in /home/feng/CS/venv/lib/python3.12/site-packages (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8) (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xt1317y2ixSS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "\n",
    "import sys, pathlib\n",
    "sys.path.insert(0, str(pathlib.Path.cwd().parents[1]))\n",
    "from finai_contest.env_stock_trading.env_stock_trading_meta import StockTradingEnv_FinRLMeta\n",
    "from finai_contest.env_stock_trading.env_stock_trading_gym_anytrading import StockTradingEnv_gym_anytrading\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mFCP1YEhi6oi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>2.733032</td>\n",
       "      <td>2.556514</td>\n",
       "      <td>2.578128</td>\n",
       "      <td>7.460152e+08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.944416</td>\n",
       "      <td>2.619212</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>2.724325</td>\n",
       "      <td>39.189999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.839302</td>\n",
       "      <td>2.887335</td>\n",
       "      <td>2.783164</td>\n",
       "      <td>2.796974</td>\n",
       "      <td>1.181608e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>2.944416</td>\n",
       "      <td>2.619212</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.781814</td>\n",
       "      <td>2.781814</td>\n",
       "      <td>39.080002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.792471</td>\n",
       "      <td>2.917055</td>\n",
       "      <td>2.773559</td>\n",
       "      <td>2.880430</td>\n",
       "      <td>1.289310e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>2.901000</td>\n",
       "      <td>2.669733</td>\n",
       "      <td>70.355544</td>\n",
       "      <td>45.847772</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.785366</td>\n",
       "      <td>2.785366</td>\n",
       "      <td>38.560001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.732131</td>\n",
       "      <td>2.776861</td>\n",
       "      <td>2.709616</td>\n",
       "      <td>2.756148</td>\n",
       "      <td>7.530488e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>2.880446</td>\n",
       "      <td>2.663669</td>\n",
       "      <td>50.429299</td>\n",
       "      <td>-30.767229</td>\n",
       "      <td>43.608042</td>\n",
       "      <td>2.772058</td>\n",
       "      <td>2.772058</td>\n",
       "      <td>43.389999</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2.782865</td>\n",
       "      <td>2.796375</td>\n",
       "      <td>2.703012</td>\n",
       "      <td>2.714720</td>\n",
       "      <td>6.735008e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>2.868583</td>\n",
       "      <td>2.679855</td>\n",
       "      <td>60.227142</td>\n",
       "      <td>-8.239059</td>\n",
       "      <td>48.358192</td>\n",
       "      <td>2.774219</td>\n",
       "      <td>2.774219</td>\n",
       "      <td>42.560001</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>176.796082</td>\n",
       "      <td>176.884314</td>\n",
       "      <td>173.599973</td>\n",
       "      <td>173.619570</td>\n",
       "      <td>7.491960e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.966005</td>\n",
       "      <td>179.988880</td>\n",
       "      <td>156.216773</td>\n",
       "      <td>65.037389</td>\n",
       "      <td>111.207660</td>\n",
       "      <td>48.878506</td>\n",
       "      <td>163.182839</td>\n",
       "      <td>153.508201</td>\n",
       "      <td>17.680000</td>\n",
       "      <td>9.832532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269</th>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.776443</td>\n",
       "      <td>177.776474</td>\n",
       "      <td>175.031343</td>\n",
       "      <td>176.629404</td>\n",
       "      <td>7.914430e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.015730</td>\n",
       "      <td>180.195302</td>\n",
       "      <td>157.878021</td>\n",
       "      <td>64.013428</td>\n",
       "      <td>108.220381</td>\n",
       "      <td>50.226096</td>\n",
       "      <td>164.140366</td>\n",
       "      <td>154.110297</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>8.468456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>175.864716</td>\n",
       "      <td>177.090219</td>\n",
       "      <td>174.649010</td>\n",
       "      <td>175.815692</td>\n",
       "      <td>6.234890e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.004570</td>\n",
       "      <td>180.775817</td>\n",
       "      <td>158.677919</td>\n",
       "      <td>64.064097</td>\n",
       "      <td>99.807304</td>\n",
       "      <td>48.433639</td>\n",
       "      <td>165.100509</td>\n",
       "      <td>154.771135</td>\n",
       "      <td>16.950001</td>\n",
       "      <td>6.345919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.707809</td>\n",
       "      <td>177.031375</td>\n",
       "      <td>174.599965</td>\n",
       "      <td>175.952925</td>\n",
       "      <td>5.977300e+07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.846506</td>\n",
       "      <td>180.938430</td>\n",
       "      <td>159.831985</td>\n",
       "      <td>62.864020</td>\n",
       "      <td>91.888029</td>\n",
       "      <td>48.191173</td>\n",
       "      <td>165.989408</td>\n",
       "      <td>155.380548</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>7.367172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>174.090179</td>\n",
       "      <td>175.717637</td>\n",
       "      <td>173.786242</td>\n",
       "      <td>174.599978</td>\n",
       "      <td>6.406230e+07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.618166</td>\n",
       "      <td>180.652247</td>\n",
       "      <td>161.472109</td>\n",
       "      <td>62.220325</td>\n",
       "      <td>79.373607</td>\n",
       "      <td>44.018874</td>\n",
       "      <td>166.776345</td>\n",
       "      <td>155.965145</td>\n",
       "      <td>17.219999</td>\n",
       "      <td>12.130734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3273 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   tic       close        high         low        open  \\\n",
       "                                                                         \n",
       "0     2009-01-02  AAPL    2.724325    2.733032    2.556514    2.578128   \n",
       "1     2009-01-05  AAPL    2.839302    2.887335    2.783164    2.796974   \n",
       "2     2009-01-06  AAPL    2.792471    2.917055    2.773559    2.880430   \n",
       "3     2009-01-07  AAPL    2.732131    2.776861    2.709616    2.756148   \n",
       "4     2009-01-08  AAPL    2.782865    2.796375    2.703012    2.714720   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "3268  2021-12-27  AAPL  176.796082  176.884314  173.599973  173.619570   \n",
       "3269  2021-12-28  AAPL  175.776443  177.776474  175.031343  176.629404   \n",
       "3270  2021-12-29  AAPL  175.864716  177.090219  174.649010  175.815692   \n",
       "3271  2021-12-30  AAPL  174.707809  177.031375  174.599965  175.952925   \n",
       "3272  2021-12-31  AAPL  174.090179  175.717637  173.786242  174.599978   \n",
       "\n",
       "            volume  day      macd     boll_ub     boll_lb      rsi_30  \\\n",
       "                                                                        \n",
       "0     7.460152e+08  4.0  0.000000    2.944416    2.619212  100.000000   \n",
       "1     1.181608e+09  0.0  0.002580    2.944416    2.619212  100.000000   \n",
       "2     1.289310e+09  1.0  0.001835    2.901000    2.669733   70.355544   \n",
       "3     7.530488e+08  2.0 -0.000728    2.880446    2.663669   50.429299   \n",
       "4     6.735008e+08  3.0 -0.000086    2.868583    2.679855   60.227142   \n",
       "...            ...  ...       ...         ...         ...         ...   \n",
       "3268  7.491960e+07  0.0  4.966005  179.988880  156.216773   65.037389   \n",
       "3269  7.914430e+07  1.0  5.015730  180.195302  157.878021   64.013428   \n",
       "3270  6.234890e+07  2.0  5.004570  180.775817  158.677919   64.064097   \n",
       "3271  5.977300e+07  3.0  4.846506  180.938430  159.831985   62.864020   \n",
       "3272  6.406230e+07  4.0  4.618166  180.652247  161.472109   62.220325   \n",
       "\n",
       "          cci_30       dx_30  close_30_sma  close_60_sma        vix  \\\n",
       "                                                                      \n",
       "0      66.666667  100.000000      2.724325      2.724325  39.189999   \n",
       "1      66.666667  100.000000      2.781814      2.781814  39.080002   \n",
       "2      45.847772  100.000000      2.785366      2.785366  38.560001   \n",
       "3     -30.767229   43.608042      2.772058      2.772058  43.389999   \n",
       "4      -8.239059   48.358192      2.774219      2.774219  42.560001   \n",
       "...          ...         ...           ...           ...        ...   \n",
       "3268  111.207660   48.878506    163.182839    153.508201  17.680000   \n",
       "3269  108.220381   50.226096    164.140366    154.110297  17.540001   \n",
       "3270   99.807304   48.433639    165.100509    154.771135  16.950001   \n",
       "3271   91.888029   48.191173    165.989408    155.380548  17.330000   \n",
       "3272   79.373607   44.018874    166.776345    155.965145  17.219999   \n",
       "\n",
       "      turbulence  \n",
       "                  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       0.000000  \n",
       "3       0.000000  \n",
       "4       0.000000  \n",
       "...          ...  \n",
       "3268    9.832532  \n",
       "3269    8.468456  \n",
       "3270    6.345919  \n",
       "3271    7.367172  \n",
       "3272   12.130734  \n",
       "\n",
       "[3273 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train_data.csv')\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "train = train[train[\"tic\"] == \"AAPL\"]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WsOLoeNcJF8Q"
   },
   "outputs": [],
   "source": [
    "env_used = \"finrl\"\n",
    "# env_used = \"gym_anytrading\"\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "\n",
    "if env_used ==\"finrl\":\n",
    "    env_kwargs = {\n",
    "        \"hmax\": 100,\n",
    "        \"initial_amount\": 1000000,\n",
    "        \"num_stock_shares\": num_stock_shares,\n",
    "        \"buy_cost_pct\": buy_cost_list,\n",
    "        \"sell_cost_pct\": sell_cost_list,\n",
    "        \"state_space\": state_space,\n",
    "        \"stock_dim\": stock_dimension,\n",
    "        \"tech_indicator_list\": INDICATORS,\n",
    "        \"action_space\": stock_dimension,\n",
    "        \"reward_scaling\": 1e-4\n",
    "    }\n",
    "\n",
    "\n",
    "    e_train_gym = StockTradingEnv_FinRLMeta(df = train, **env_kwargs)\n",
    "\n",
    "elif env_used ==\"gym_anytrading\":\n",
    "    env_kwargs = {\n",
    "    \"hmax\": np.inf,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": 2*stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"window_size\": 30\n",
    "    }\n",
    "\n",
    "    e_train_gym = StockTradingEnv_gym_anytrading(df = train, **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
   },
   "outputs": [],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aapl = train[train[\"tic\"] == \"aapl\"]\n",
    "# train_gym_anytrade = pd.DataFrame()\n",
    "# train_gym_anytrade['Time'] = pd.to_datetime(train_aapl['date'])  # Convert to datetime\n",
    "# train_gym_anytrade['Open'] = train_aapl['open']\n",
    "# train_gym_anytrade['High'] = train_aapl['high']\n",
    "# train_gym_anytrade['Low'] = train_aapl['low']\n",
    "# train_gym_anytrade['Close'] = train_aapl['close']\n",
    "# train_gym_anytrade['Volume'] = train_aapl['volume']\n",
    "# train_aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "# import gym_anytrading\n",
    "# env_train = gym.make(\n",
    "#     'stocks-v0',\n",
    "#     df=train_gym_anytrade,\n",
    "#     window_size=30,\n",
    "#     frame_bound=(30, len(train_gym_anytrade))\n",
    "# )\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# def get_sb_env(self):\n",
    "#     e = DummyVecEnv([lambda: self])\n",
    "#     obs = e.reset()\n",
    "#     return e, obs\n",
    "# # Patch the method\n",
    "# env_train = env_train.env\n",
    "# env_train = env_train.env\n",
    "# env_train.get_sb_env = get_sb_env.__get__(env_train)\n",
    "# env_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "364PsqckttcQ"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = False\n",
    "if_using_ddpg = False\n",
    "if_using_ppo = True\n",
    "if_using_td3 = False\n",
    "if_using_sac = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feng/CS/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
   },
   "outputs": [],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zjCWfgsg3sVa"
   },
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "M2YadjfnLwgt"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "# if if_using_ddpg:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/ddpg'\n",
    "#   new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tCDa78rqfO_a"
   },
   "outputs": [],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ne6M2R-WvrUQ"
   },
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "y5D5PFUhMzSV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feng/CS/venv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Gt8eIQKYM4G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 511          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.015772682  |\n",
      "|    reward_max      | 0.2502969    |\n",
      "|    reward_mean     | 0.0014232412 |\n",
      "|    reward_min      | -0.096222244 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 425          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014315646 |\n",
      "|    clip_fraction        | 0.00425      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0139      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000117    |\n",
      "|    reward               | -0.03512826  |\n",
      "|    reward_max           | 5.3997946    |\n",
      "|    reward_mean          | 0.044183467  |\n",
      "|    reward_min           | -5.9602046   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00427      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 432         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002376146 |\n",
      "|    clip_fraction        | 0.0167      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    reward               | 1.6008418   |\n",
      "|    reward_max           | 11.858458   |\n",
      "|    reward_mean          | 0.03949251  |\n",
      "|    reward_min           | -14.330505  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.47        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 441          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.788637e-05 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.85         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000186     |\n",
      "|    reward               | 0.94431525   |\n",
      "|    reward_max           | 16.25157     |\n",
      "|    reward_mean          | 0.09723091   |\n",
      "|    reward_min           | -17.587791   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.26         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 446          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038035912 |\n",
      "|    clip_fraction        | 0.00928      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00789      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000865    |\n",
      "|    reward               | 0.07678009   |\n",
      "|    reward_max           | 18.749634    |\n",
      "|    reward_mean          | 0.1352472    |\n",
      "|    reward_min           | -19.490002   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 27.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039786    |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0475       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    reward               | 0.011041672  |\n",
      "|    reward_max           | 0.21518807   |\n",
      "|    reward_mean          | 0.0013374635 |\n",
      "|    reward_min           | -0.16899826  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 46.2         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 455           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038207558 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00854      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | 8.3e-05       |\n",
      "|    reward               | -0.011267457  |\n",
      "|    reward_max           | 2.8981345     |\n",
      "|    reward_mean          | 0.014178918   |\n",
      "|    reward_min           | -3.7023194    |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.0124        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 457           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0009087891  |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.464         |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | 5.46e-05      |\n",
      "|    reward               | -0.0008212932 |\n",
      "|    reward_max           | 1.3307798     |\n",
      "|    reward_mean          | 0.008817652   |\n",
      "|    reward_min           | -1.762657     |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.761         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 460          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00348433   |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.144        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00094     |\n",
      "|    reward               | -0.094456084 |\n",
      "|    reward_max           | 0.51349616   |\n",
      "|    reward_mean          | 0.003416754  |\n",
      "|    reward_min           | -0.45931083  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.227        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046569747 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0122       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    reward               | 0.010547567  |\n",
      "|    reward_max           | 0.5285204    |\n",
      "|    reward_mean          | 0.0045578433 |\n",
      "|    reward_min           | -0.6189819   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.035        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003480123 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    reward               | -2.450557   |\n",
      "|    reward_max           | 6.7920513   |\n",
      "|    reward_mean          | 0.031015059 |\n",
      "|    reward_min           | -8.081129   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0534      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 464          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007977298 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.47         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | 5.32e-05     |\n",
      "|    reward               | 0.2016304    |\n",
      "|    reward_max           | 8.6658325    |\n",
      "|    reward_mean          | 0.05248185   |\n",
      "|    reward_min           | -9.414897    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 5.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012343102 |\n",
      "|    clip_fraction        | 0.00142      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.021        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.14         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000309    |\n",
      "|    reward               | -0.022824114 |\n",
      "|    reward_max           | 21.929321    |\n",
      "|    reward_mean          | 0.1646094    |\n",
      "|    reward_min           | -22.950426   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 9.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.425406e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.0985       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 24.9         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000248    |\n",
      "|    reward               | -5.5265284   |\n",
      "|    reward_max           | 7.569364     |\n",
      "|    reward_mean          | 0.03176229   |\n",
      "|    reward_min           | -10.398069   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 62.4         |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5756618.70\n",
      "total_reward: 4756618.70\n",
      "total_cost: 7879.57\n",
      "total_trades: 3244\n",
      "Sharpe: 0.883\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004683027 |\n",
      "|    clip_fraction        | 0.0103      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.72        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    reward               | -0.10125618 |\n",
      "|    reward_max           | 30.516617   |\n",
      "|    reward_mean          | 0.20090584  |\n",
      "|    reward_min           | -32.303745  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 8.22        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 467           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 70            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0004931427  |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.156         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 65.9          |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000115     |\n",
      "|    reward               | 4.1316904e-05 |\n",
      "|    reward_max           | 14.985376     |\n",
      "|    reward_mean          | 0.10864757    |\n",
      "|    reward_min           | -15.530858    |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 108           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026155834 |\n",
      "|    clip_fraction        | 0.00142      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00857     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.74         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -5.45e-05    |\n",
      "|    reward               | -0.1416708   |\n",
      "|    reward_max           | 1.5241746    |\n",
      "|    reward_mean          | 0.0053216806 |\n",
      "|    reward_min           | -1.2240868   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 26.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062230006 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.099        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00281     |\n",
      "|    reward               | 0.0002674712 |\n",
      "|    reward_max           | 4.666762     |\n",
      "|    reward_mean          | 0.035361957  |\n",
      "|    reward_min           | -5.1772704   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.305        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 469           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 82            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0010280403  |\n",
      "|    clip_fraction        | 0.0042        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.06          |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | 0.000171      |\n",
      "|    reward               | -0.0071517075 |\n",
      "|    reward_max           | 0.18232343    |\n",
      "|    reward_mean          | 0.00037876653 |\n",
      "|    reward_min           | -0.23835048   |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 3.36          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062540444 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0203      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00325     |\n",
      "|    reward               | 0.020532887  |\n",
      "|    reward_max           | 0.5021688    |\n",
      "|    reward_mean          | 0.002469855  |\n",
      "|    reward_min           | -0.34801102  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0273       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 470           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 91            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008321914   |\n",
      "|    clip_fraction        | 0.0758        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00977      |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -0.00416      |\n",
      "|    reward               | -0.0057369457 |\n",
      "|    reward_max           | 4.789129      |\n",
      "|    reward_mean          | 0.03468536    |\n",
      "|    reward_min           | -4.762022     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.0659        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 471           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 95            |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041930508 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.38          |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -6.16e-06     |\n",
      "|    reward               | 0.21922228    |\n",
      "|    reward_max           | 1.2321314     |\n",
      "|    reward_mean          | 0.0034746584  |\n",
      "|    reward_min           | -1.4765258    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 3.63          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 472          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051770695 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0903       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    reward               | 0.10414774   |\n",
      "|    reward_max           | 7.7737484    |\n",
      "|    reward_mean          | 0.060086846  |\n",
      "|    reward_min           | -8.175387    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.188        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 470           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 104           |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9357802e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.228         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.81          |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | 3.21e-06      |\n",
      "|    reward               | 0.0070607173  |\n",
      "|    reward_max           | 23.343763     |\n",
      "|    reward_mean          | 0.17873271    |\n",
      "|    reward_min           | -25.170856    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 9.22          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.042367e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.344        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 36.1         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | 9.06e-05     |\n",
      "|    reward               | -0.16494216  |\n",
      "|    reward_max           | 2.735077     |\n",
      "|    reward_mean          | 0.015293982  |\n",
      "|    reward_min           | -1.9872713   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 74.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 112          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011107372 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.461        |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00015     |\n",
      "|    reward               | 0.62365115   |\n",
      "|    reward_max           | 24.113914    |\n",
      "|    reward_mean          | 0.17431767   |\n",
      "|    reward_min           | -25.48653    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 1.02         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 471           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 117           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012342184 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.357         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 32            |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.000271     |\n",
      "|    reward               | 10.757049     |\n",
      "|    reward_max           | 42.45808      |\n",
      "|    reward_mean          | 0.18901944    |\n",
      "|    reward_min           | -37.694366    |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 69.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 471           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 121           |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019953758 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.3           |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 63.7          |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000342     |\n",
      "|    reward               | -0.6828778    |\n",
      "|    reward_max           | 31.267786     |\n",
      "|    reward_mean          | 0.13461709    |\n",
      "|    reward_min           | -44.038937    |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 106           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 472          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006819101 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 38.8         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    reward               | 0.1328736    |\n",
      "|    reward_max           | 38.067707    |\n",
      "|    reward_mean          | 0.27015617   |\n",
      "|    reward_min           | -39.709156   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001199096 |\n",
      "|    clip_fraction        | 4.88e-05    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 57          |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    reward               | 1.6801089   |\n",
      "|    reward_max           | 9.855776    |\n",
      "|    reward_mean          | 0.027732987 |\n",
      "|    reward_min           | -14.140536  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "day: 3272, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7286748.57\n",
      "total_reward: 6286748.57\n",
      "total_cost: 6811.58\n",
      "total_trades: 3246\n",
      "Sharpe: 0.905\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 472          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054600053 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.509        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.04         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00542     |\n",
      "|    reward               | -0.16287975  |\n",
      "|    reward_max           | 40.72363     |\n",
      "|    reward_mean          | 0.28518403   |\n",
      "|    reward_min           | -42.73523    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 18.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 472           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 138           |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041298996 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.541         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 89.4          |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000514     |\n",
      "|    reward               | 0.020606594   |\n",
      "|    reward_max           | 54.048794     |\n",
      "|    reward_mean          | 0.41026992    |\n",
      "|    reward_min           | -56.526814    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 177           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 472           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 142           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023197086 |\n",
      "|    clip_fraction        | 0.000195      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.227         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 160           |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.000164     |\n",
      "|    reward               | -3.4503758    |\n",
      "|    reward_max           | 6.6566596     |\n",
      "|    reward_mean          | 0.033212326   |\n",
      "|    reward_min           | -5.8214006    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 359           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 472          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029061656 |\n",
      "|    clip_fraction        | 0.00859      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.21         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    reward               | 0.6244552    |\n",
      "|    reward_max           | 46.778072    |\n",
      "|    reward_mean          | 0.3333543    |\n",
      "|    reward_min           | -49.145645   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.48         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004795091 |\n",
      "|    clip_fraction        | 0.0208      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 177         |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00444    |\n",
      "|    reward               | -8.858533   |\n",
      "|    reward_max           | 49.817562   |\n",
      "|    reward_mean          | 0.2100387   |\n",
      "|    reward_min           | -52.136517  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 255         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 156          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012999371 |\n",
      "|    clip_fraction        | 0.00571      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.526        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 62.5         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    reward               | -0.4815533   |\n",
      "|    reward_max           | 36.797752    |\n",
      "|    reward_mean          | 0.18875019   |\n",
      "|    reward_min           | -34.941334   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 172          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014984646 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 73.3         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    reward               | 0.028469142  |\n",
      "|    reward_max           | 50.54447     |\n",
      "|    reward_mean          | 0.36640987   |\n",
      "|    reward_min           | -52.78521    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024383971 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.52         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 97.2         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000736    |\n",
      "|    reward               | -0.89328724  |\n",
      "|    reward_max           | 13.93683     |\n",
      "|    reward_mean          | 0.054344986  |\n",
      "|    reward_min           | -19.658493   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 304          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007916168 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.785        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000894    |\n",
      "|    reward               | 0.080882765  |\n",
      "|    reward_max           | 51.414364    |\n",
      "|    reward_mean          | 0.34872624   |\n",
      "|    reward_min           | -53.806065   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 43.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 174          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006359486 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.643        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 127          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000513    |\n",
      "|    reward               | -0.014844968 |\n",
      "|    reward_max           | 52.2953      |\n",
      "|    reward_mean          | 0.39666623   |\n",
      "|    reward_min           | -54.71171    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 275          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007414279 |\n",
      "|    clip_fraction        | 0.00786      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.507        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    reward               | 1.1585158    |\n",
      "|    reward_max           | 8.913522     |\n",
      "|    reward_mean          | 0.045228813  |\n",
      "|    reward_min           | -7.8000693   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 331          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013405001 |\n",
      "|    clip_fraction        | 0.0043       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.95         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000205    |\n",
      "|    reward               | -0.5053514   |\n",
      "|    reward_max           | 55.77313     |\n",
      "|    reward_mean          | 0.40224987   |\n",
      "|    reward_min           | -58.400677   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 9.86         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 187          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010649537 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.586        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 101          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -6.67e-06    |\n",
      "|    reward               | 0.52879447   |\n",
      "|    reward_max           | 59.069374    |\n",
      "|    reward_mean          | 0.2857556    |\n",
      "|    reward_min           | -61.808327   |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 369          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021725849 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.6          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 84.3         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    reward               | -1.452735    |\n",
      "|    reward_max           | 38.48186     |\n",
      "|    reward_mean          | 0.19873318   |\n",
      "|    reward_min           | -41.667007   |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 253          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 471           |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 195           |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054628437 |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.658         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 87.8          |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -0.000277     |\n",
      "|    reward               | -0.33214977   |\n",
      "|    reward_max           | 64.60096      |\n",
      "|    reward_mean          | 0.47909132    |\n",
      "|    reward_min           | -67.603455    |\n",
      "|    std                  | 0.984         |\n",
      "|    value_loss           | 183           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 199          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023134854 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.486        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 245          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    reward               | 1.2708137    |\n",
      "|    reward_max           | 17.609825    |\n",
      "|    reward_mean          | 0.088785976  |\n",
      "|    reward_min           | -24.879011   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 508          |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11573433.53\n",
      "total_reward: 10573433.53\n",
      "total_cost: 4284.21\n",
      "total_trades: 3250\n",
      "Sharpe: 0.957\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 204          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016548282 |\n",
      "|    clip_fraction        | 0.00488      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000189    |\n",
      "|    reward               | -0.4268863   |\n",
      "|    reward_max           | 65.01833     |\n",
      "|    reward_mean          | 0.44339377   |\n",
      "|    reward_min           | -68.00887    |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 79.9         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 471           |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 208           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0015187529  |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.582         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 217           |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -0.000173     |\n",
      "|    reward               | -0.0101765115 |\n",
      "|    reward_max           | 71.63049      |\n",
      "|    reward_mean          | 0.5505706     |\n",
      "|    reward_min           | -74.96072     |\n",
      "|    std                  | 0.986         |\n",
      "|    value_loss           | 441           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037557674 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.247        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 291          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    reward               | 1.3164127    |\n",
      "|    reward_max           | 11.780491    |\n",
      "|    reward_mean          | 0.07359613   |\n",
      "|    reward_min           | -10.596756   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 652          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=100000) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "C6AidlWyvwzm"
   },
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo_\"+env_used) if if_using_ppo else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "JSAHhV4Xc-bh"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "# if if_using_td3:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/td3'\n",
    "#   new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_td3.set_logger(new_logger_td3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OSRxNYAxdKpU"
   },
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OkJV6V_mv2hw"
   },
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xwOhVjqRkCdM"
   },
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 128,\n",
    "#     \"buffer_size\": 100000,\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"learning_starts\": 100,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "# if if_using_sac:\n",
    "#   # set up logger\n",
    "#   tmp_path = RESULTS_DIR + '/sac'\n",
    "#   new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "#   # Set new logger\n",
    "#   model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "K8RSdKCckJyH"
   },
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_SpZoQgPv7GO"
   },
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
