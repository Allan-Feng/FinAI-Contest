

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Task 2: FinRL-AlphaSeek for Crypto Trading &mdash; FinRL Contest Documentation 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=e8ce10b2" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Task 3 Open FinLLM Leaderboard – Models with ReFT" href="task3.html" />
    <link rel="prev" title="Task 1 FinRL-DeepSeek for Stock Trading" href="task1.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            FinRL Contest Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/finrl.html">FinRL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/env.html">Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/agent.html">Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Baseline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../baseline/market.html">Market Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../baseline/regression.html">Regression Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FinRL Contest 2025</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="task1.html">Task 1 FinRL-DeepSeek for Stock Trading</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Task 2: FinRL-AlphaSeek for Crypto Trading</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#detailed-description">Detailed Description</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#first-stage-factor-mining-and-alpha-101">First Stage: Factor Mining and Alpha 101</a></li>
<li class="toctree-l3"><a class="reference internal" href="#second-stage-reinforcement-learning-for-crypto-trading">Second Stage: Reinforcement Learning for Crypto Trading</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dataset">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#submission-guidelines">Submission Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-criteria">Evaluation Criteria</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="task3.html">Task 3 Open FinLLM Leaderboard – Models with ReFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="task4.html">Task 4 Open FinRL Leaderboard – Digital Regulatory Reporting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FinRL Contest 2024</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../finrl2024/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2024/task1.html">Task 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2024/task2.html">Task 2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FinRL Contest 2023</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../finrl2023/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2023/task1.html">Task 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2023/task2.html">Task 2</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">FinRL Contest Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Task 2: FinRL-AlphaSeek for Crypto Trading</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Open-Finance-Lab/FinRL-Contest/blob/main/docs/source/finrl2025/task2.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="task-2-finrl-alphaseek-for-crypto-trading">
<h1>Task 2: FinRL-AlphaSeek for Crypto Trading<a class="headerlink" href="#task-2-finrl-alphaseek-for-crypto-trading" title="Link to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>This task focuses on developing robust and effective trading agents for cryptocurrencies through factor mining and ensemble learning. Participants will explore useful factors and ensemble methods specifically tailored for crypto trading. For this year’s competition, the factor mining stage has been expanded, allowing participants to design their own factor mining models to generate powerful trading signals.</p>
<p>Participants are encouraged to apply various techniques to factor engineering, design component models, and use innovative methods to increase the diversity of component models in the ensemble. Additionally, participants need to specify the state space, action space, and reward function in their trading environment. The final model should seamlessly interact with the provided trading environment.</p>
<p>For reference, an example ensemble method using the majority voting approach is provided in the tutorial: <a class="reference external" href="https://github.com/Open-Finance-Lab/FinRL_Contest_2024/tree/main/Tutorials/Task_1_tutorial">Task 1 Crypto Trading Ensemble</a>.</p>
</section>
<section id="detailed-description">
<h2>Detailed Description<a class="headerlink" href="#detailed-description" title="Link to this heading"></a></h2>
<section id="first-stage-factor-mining-and-alpha-101">
<h3>First Stage: Factor Mining and Alpha 101<a class="headerlink" href="#first-stage-factor-mining-and-alpha-101" title="Link to this heading"></a></h3>
<dl>
<dt><strong>What is Alpha 101?</strong></dt><dd><p>Alpha 101 refers to a set of formulaic alphas used to generate trading signals from financial data. These alphas leverage features derived from historical price and volume data to identify profitable trading opportunities.</p>
<p>In this task, factor mining involves deriving meaningful financial indicators from limit order book (LOB) data and using techniques like recurrent neural networks (RNNs) to generate robust predictive signals.</p>
</dd>
<dt><strong>Architecture</strong></dt><dd><p>The architecture consists of an ensemble of reinforcement learning agents trained using massively parallel simulations. The system integrates deep reinforcement learning (DRL) with a supervised learning stage where Alpha101 features are extracted and processed using RNN-based networks.</p>
<p>The pipeline includes:</p>
<ul class="simple">
<li><p><strong>Factor Mining</strong>: Weak Alpha101 signals processed through LSTM+GRU networks to generate strong factors.</p></li>
<li><p><strong>Agent Training</strong>: DRL agents trained in a parallel market environment to optimize trading decisions.</p></li>
<li><p><strong>Ensemble Method</strong>: A combination of agents using majority voting or weighted action averaging to enhance robustness.</p></li>
</ul>
</dd>
<dt><strong>Design Rationale</strong></dt><dd><p>This design aims to mitigate policy instability and sampling bottlenecks, two major challenges in financial reinforcement learning.</p>
<p>Factor mining provides stronger signals to RL agents, improving decision quality.</p>
<p>Running thousands of parallel simulations on GPUs speeds up training, allowing agents to adapt to volatile markets.</p>
</dd>
</dl>
</section>
<section id="second-stage-reinforcement-learning-for-crypto-trading">
<h3>Second Stage: Reinforcement Learning for Crypto Trading<a class="headerlink" href="#second-stage-reinforcement-learning-for-crypto-trading" title="Link to this heading"></a></h3>
<dl>
<dt><strong>RL Setting for Crypto Trading</strong></dt><dd><p>The crypto trading task is modeled as a Markov Decision Process (MDP), where:</p>
<ul class="simple">
<li><p><strong>State space</strong> includes order book features, price data, and technical indicators.</p></li>
<li><p><strong>Action space</strong> consists of discrete trade actions (buy, sell, hold) determined by DQN-based models.</p></li>
<li><p><strong>Reward function</strong> is based on profit and loss calculations, emphasizing risk-adjusted returns.</p></li>
</ul>
<p>Reinforcement learning agents learn to optimize trading strategies by interacting with a simulated market environment.</p>
</dd>
<dt><strong>Parallel Environment</strong></dt><dd><p>The parallel environment consists of thousands of simulated trading environments running concurrently on GPUs.</p>
<p>This approach enhances sample efficiency and speeds up convergence, making it feasible to train complex trading agents in a short period.</p>
<p>A market replay simulator is used to generate training samples from historical data, ensuring the agent learns from realistic market conditions.</p>
</dd>
<dt><strong>Agent Selection Rationale</strong></dt><dd><p>Different agents have varying strengths:</p>
<ul class="simple">
<li><p><strong>PPO (Proximal Policy Optimization)</strong>: Stability in training, suitable for continuous action spaces.</p></li>
<li><p><strong>SAC (Soft Actor-Critic)</strong>: Efficient exploration, performs well in complex environments.</p></li>
<li><p><strong>DDPG (Deep Deterministic Policy Gradient)</strong>: Works well with continuous control tasks.</p></li>
<li><p><strong>DQN and Variants (Double DQN, Dueling DQN)</strong>: Well-suited for discrete action spaces, effective in high-frequency trading.</p></li>
</ul>
<p>The diversity of agents allows the ensemble method to leverage multiple perspectives on market conditions, improving robustness.</p>
</dd>
<dt><strong>Increasing Agent Diversity</strong></dt><dd><ul class="simple">
<li><p>Training different models on varied market scenarios</p></li>
<li><p>Using different subsets of historical data to train agents on different market regimes</p></li>
<li><p>Applying KL divergence penalties in the loss function to encourage diverse policy behaviors</p></li>
<li><p>Experimenting with different hyperparameters to ensure agents do not converge to similar strategies</p></li>
</ul>
</dd>
</dl>
</section>
</section>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading"></a></h2>
<p>A dataset containing second-level Limit Order Book (LOB) data for Bitcoin is provided. Please download it from <a class="reference external" href="https://drive.google.com/drive/folders/1ExVPS1d77oPOHXMRYdtKpdEC0PycthKW?usp=sharing">here</a>. This dataset is essential for training both supervised learning models and reinforcement learning agents.</p>
</section>
<section id="submission-guidelines">
<h2>Submission Guidelines<a class="headerlink" href="#submission-guidelines" title="Link to this heading"></a></h2>
<p>Participants should submit:</p>
<ol class="arabic simple">
<li><p>Trained models and the scripts to load and test them</p></li>
<li><p>A README explaining their submission and how it should be evaluated</p></li>
<li><p>Code for factor mining, ensemble methods, and any modifications to the trading environment</p></li>
</ol>
</section>
<section id="evaluation-criteria">
<h2>Evaluation Criteria<a class="headerlink" href="#evaluation-criteria" title="Link to this heading"></a></h2>
<p>Models will be evaluated based on:</p>
<ul class="simple">
<li><p>Cumulative Return</p></li>
<li><p>Sharpe Ratio</p></li>
<li><p>Win/Loss Ratio</p></li>
</ul>
<p>By combining factor mining, reinforcement learning, and ensemble methods, this task aims to advance state-of-the-art crypto trading strategies through AI-driven approaches.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="task1.html" class="btn btn-neutral float-left" title="Task 1 FinRL-DeepSeek for Stock Trading" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="task3.html" class="btn btn-neutral float-right" title="Task 3 Open FinLLM Leaderboard – Models with ReFT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, FinRL Contest.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>