

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Task 1 FinRL-DeepSeek for Stock Trading &mdash; FinRL Contest Documentation 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=e8ce10b2" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f2a433a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Task 2: FinRL-AlphaSeek for Crypto Trading" href="task2.html" />
    <link rel="prev" title="Overview" href="overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            FinRL Contest Documentation
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/data.html">Market Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/env.html">Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/agent.html">Agent</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Baseline</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../baseline/market.html">Market Index</a></li>
<li class="toctree-l1"><a class="reference internal" href="../baseline/regression.html">Regression Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FinRL Contest 2025</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Task 1 FinRL-DeepSeek for Stock Trading</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#task-overview">Task Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#why-this-matters">Why This Matters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#datasets">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#starter-kit-training-environments">Starter Kit: Training &amp; Environments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training-commands">Training Commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="#environment-files">Environment Files</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring-training">Monitoring Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-example">Evaluation (Example)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="task2.html">Task 2: FinRL-AlphaSeek for Crypto Trading</a></li>
<li class="toctree-l1"><a class="reference internal" href="task3.html">Task 3 Open FinLLM Leaderboard – Models with ReFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="task4.html">Task 4 Open FinRL Leaderboard – Digital Regulatory Reporting</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FinRL Contest 2024</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../finrl2024/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2024/task1.html">Task 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2024/task2.html">Task 2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FinRL Contest 2023</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../finrl2023/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2023/task1.html">Task 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="../finrl2023/task2.html">Task 2</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">FinRL Contest Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Task 1 FinRL-DeepSeek for Stock Trading</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/Open-Finance-Lab/FinRL-Contest/blob/main/docs/source/finrl2025/task1.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="task-1-finrl-deepseek-for-stock-trading">
<h1>Task 1 FinRL-DeepSeek for Stock Trading<a class="headerlink" href="#task-1-finrl-deepseek-for-stock-trading" title="Link to this heading"></a></h1>
<p>Welcome to Task 1: FinRL-DeepSeek for Stock Trading, a competition that challenges you to build next-generation trading agents by combining FinRL and LLMs. Your mission: train agents that can trade stocks using both market data and financial news while effectively managing risk. This task builds on the FinRL-DeepSeek <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> — a novel integration of LLMs and  RL algorithms designed for real-world stock trading scenarios.</p>
<section id="task-overview">
<h2>Task Overview<a class="headerlink" href="#task-overview" title="Link to this heading"></a></h2>
<p>In this task, participants are invited to develop stock trading agents that integrate LLM-generated signals in FinRL using both market and news data. Building on the <a class="reference external" href="https://github.com/benstaf/FinRL_DeepSeek">FinRL-DeepSeek</a>, participants can explore a variety of directions, including but not limited to:</p>
<ul class="simple">
<li><p>Designing new LLM prompting strategies to extract trading or risk signals from financial news</p></li>
<li><p>Exploring novel ways to inject LLM-generated signals into the RL pipeline (e.g., into policy networks, reward shaping, or environment dynamics)</p></li>
<li><p>Applying alternative RL algorithms, such as Generalized Reward Policy Optimization (GRPO).</p></li>
<li><p>Investigating computationally intensive approaches, including adapting instruction-tuned variants of the DeepSeek R1 training methodology to this stock trading task</p></li>
</ul>
<p>Participants are encouraged to propose creative improvements and extensions that further advance the hybrid LLM-RL paradigm in financial decision-making.</p>
</section>
<section id="why-this-matters">
<h2>Why This Matters<a class="headerlink" href="#why-this-matters" title="Link to this heading"></a></h2>
<p>Modern RL trading agents often rely on structured price data and overlook unstructured signals like news. They also tend to optimize returns without controlling risk — leading to poor performance in volatile markets.</p>
<p>FinRL-DeepSeek introduces a hybrid solution with:</p>
<ul class="simple">
<li><p>Dual LLM Guidance: One LLM for trading recommendations, another for risk scoring</p></li>
<li><p>CVaR-PPO Integration: Risk-constrained learning to cap losses while pursuing gains</p></li>
<li><p>Action Modulation: Agent actions are scaled by LLM recommendations</p></li>
<li><p>Reward Adjustment: High-risk actions are penalized using LLM-derived risk scores</p></li>
</ul>
<p>This is the first time CVaR-PPO has been adapted to stock trading with LLMs. We invite you to explore, improve, and extend it.</p>
</section>
<section id="datasets">
<h2>Datasets<a class="headerlink" href="#datasets" title="Link to this heading"></a></h2>
<p>The <a class="reference external" href="https://huggingface.co/datasets/Zihan1004/FNSPID">Financial News and Stock Price Integration Dataset (FNSPID)</a> contains historical stock prices and over 15 million time-aligned financial news articles related to Nasdaq-listed companies, spanning from 1999 to 2023. We include a processed <a class="reference external" href="https://huggingface.co/datasets/benstaf/nasdaq_2013_2023">subset of this dataset</a> as an example for participants to explore and build their solutions.</p>
<p>Participants can also incorporate additional public data sources, such as Twitter, or develop their own scraping and API-based agents. Some teams may focus on improving the dataset and news processing pipeline, while others may concentrate on designing better trading agents.</p>
</section>
<section id="starter-kit-training-environments">
<h2>Starter Kit: Training &amp; Environments<a class="headerlink" href="#starter-kit-training-environments" title="Link to this heading"></a></h2>
<p>This <a class="reference external" href="https://github.com/Open-Finance-Lab/FinRL_Contest_2025/tree/main/Task_1_FinRL_DeepSeek_Stock">starter kit</a> includes training scripts and environment files for PPO-based and CPPO-based stock trading agents, including LLM-enhanced versions. Follow the instructions below to get started.</p>
<section id="training-commands">
<h3>Training Commands<a class="headerlink" href="#training-commands" title="Link to this heading"></a></h3>
<p>You can train different types of trading agents using the following scripts. Each script corresponds to a specific reinforcement learning setup, ranging from standard PPO to LLM-enhanced and risk-sensitive configurations.</p>
<p>To train the various models, follow the instructions below:</p>
<table class="docutils align-default" id="id3">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Model Types and Training Scripts</span><a class="headerlink" href="#id3" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model Type</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Training Script / Command</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PPO</p></td>
<td><p>Standard Proximal Policy Optimization (no LLM)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nohup</span> <span class="pre">mpirun</span> <span class="pre">--allow-run-as-root</span> <span class="pre">-np</span> <span class="pre">8</span> <span class="pre">python</span> <span class="pre">train_ppo.py</span> <span class="pre">&gt;</span> <span class="pre">output_ppo.log</span> <span class="pre">2&gt;&amp;1</span> <span class="pre">&amp;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>CPPO</p></td>
<td><p>Conditional Value-at-Risk PPO (risk-sensitive)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train_cppo.py</span></code></p></td>
</tr>
<tr class="row-even"><td><p>PPO-DeepSeek</p></td>
<td><p>PPO with LLM-enhanced trading signals</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train_ppo_llm.py</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>CPPO-DeepSeek</p></td>
<td><p>CPPO with LLM-based recommendations and risk</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train_cppo_llm_risk.py</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="environment-files">
<h3>Environment Files<a class="headerlink" href="#environment-files" title="Link to this heading"></a></h3>
<p>Each training script corresponds to a specific environment implementation:</p>
<table class="docutils align-default" id="id4">
<caption><span class="caption-number">Table 2 </span><span class="caption-text">Environment Files and Usage</span><a class="headerlink" href="#id4" title="Link to this table"></a></caption>
<colgroup>
<col style="width: 40.0%" />
<col style="width: 60.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Environment File</p></th>
<th class="head"><p>Used In</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">env_stocktrading.py</span></code></p></td>
<td><p>PPO, CPPO (standard FinRL environment)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">env_stocktrading_llm.py</span></code> or <code class="docutils literal notranslate"><span class="pre">env_stocktrading_llm_01.py</span></code></p></td>
<td><p>PPO-DeepSeek (LLM-modulated trading)</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">env_stocktrading_llm_risk.py</span></code> or <code class="docutils literal notranslate"><span class="pre">env_stocktrading_llm_risk_01.py</span></code></p></td>
<td><p>CPPO-DeepSeek (LLM + risk scoring)</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="monitoring-training">
<h2>Monitoring Training<a class="headerlink" href="#monitoring-training" title="Link to this heading"></a></h2>
<p>Each training script outputs logs (e.g., output_ppo.log). Key metrics to monitor include:</p>
<ul class="simple">
<li><p>AverageEpRet: Average episode return</p></li>
<li><p>KL: KL divergence for policy update stability</p></li>
<li><p>ClipFrac: Fraction of clipped policy updates</p></li>
</ul>
<p>Use these metrics to track learning progress and tune hyperparameters accordingly.</p>
</section>
<section id="evaluation-example">
<h2>Evaluation (Example)<a class="headerlink" href="#evaluation-example" title="Link to this heading"></a></h2>
<p>We provide an example evaluation workflow for the trading period 2019–2023, implemented in the <a class="reference external" href="https://colab.research.google.com/github/benstaf/FinRL_DeepSeek/blob/main/FinRL_DeepSeek_backtesting.ipynb#scrollTo=7r6aAYR1jOdN">FinRL_DeepSeek_backtest.ipynb (Colab notebook)</a>. This example uses the following metrics to assess both return and risk-adjusted performance:</p>
<ul class="simple">
<li><p>Information Ratio – Measures risk-adjusted return relative to a benchmark</p></li>
<li><p>Conditional Value at Risk (CVaR) – Quantifies expected losses in extreme downside scenarios</p></li>
<li><p>Rachev Ratio – Captures the balance between upside potential and downside risk</p></li>
<li><p>Outperformance Frequency – Measures how often the strategy outperforms the benchmark in rolling windows</p></li>
</ul>
<p>Note: This evaluation setup is provided as a reference. Participants are encouraged to explore additional metrics or adapt the backtesting pipeline to fit their model’s characteristics.</p>
<p><strong>References</strong></p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Mostapha Benhenda. 2025. FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for Trading Agents. arXiv preprint arXiv:2502.07393 (2025).</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="overview.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="task2.html" class="btn btn-neutral float-right" title="Task 2: FinRL-AlphaSeek for Crypto Trading" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, FinRL Contest.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>